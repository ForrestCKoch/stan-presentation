{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Probabilistic Programming with STAN\"\n",
        "author: \"Forrest Koch\"\n",
        "format: \n",
        "    revealjs:\n",
        "        theme: dark\n",
        "        transition: slide\n",
        "        background-transition: fade\n",
        "title-slide-attributes:\n",
        "    data-background-image: logo_tm.png\n",
        "    data-background-size: contain\n",
        "    data-background-opacity: \"0.65\"\n",
        "    \n",
        "editor: source\n",
        "---"
      ],
      "id": "013d5389"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Disclaimer* {background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "I am by no means an expert in Bayesian Inference. I would describe myself\n",
        "Bayesian \"enthusiast\" at best.\n",
        "\n",
        "## Aims {background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "- To encourage you to consider Bayesian approaches for your analyses.\n",
        "- To put STAN on your radar as a flexible and powerful tool for Bayesian inference.\n",
        "- Give an overview of how STAN works.\n",
        "- Provide some examples of how it can be used.\n",
        "\n",
        "## Overview {background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "- Why Bayes?\n",
        "- Why STAN?\n",
        "- How does it work?\n",
        "- Some Examples.\n",
        "\n",
        "## The Canonical Formula {background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "Bayes Rule:\n",
        "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
        "\n",
        "![](Thomas_Bayes3.jpg){.absolute bottom=80 left=200 height=\"45%\"}\n",
        "![](Laplace.jpg){.absolute bottom=80 right=200 height=\"45%\"}\n",
        "\n",
        ":::{style=\"margin-top: 350px\"}\n",
        "<center><small>Thomas Bayes (1701--1761; left) and Pierre-Simon Laplace (1749--1827; right)</small></center>\n",
        ":::\n",
        "## In practice\n",
        "\n",
        "$$\\underbrace{P(\\boldsymbol{\\theta}|x)}_{\\text{posterior}}\\propto \\overbrace{\\mathcal{L}(x|\\boldsymbol{\\theta})}^{\\text{Likelihood}}\\underbrace{P(\\boldsymbol{\\theta})}_{\\text{prior}}$$\n",
        "\n",
        "- $x$ is the observed data\n",
        "- $\\boldsymbol{\\theta}$ are the parameters of interest\n",
        "- Note 1: $P(x)$ is not tractable, but it is constant\n",
        "- Note 2: A uniform prior results in $P(\\boldsymbol{\\theta}|x)\\propto \\mathcal{L}(x|\\boldsymbol{\\theta})$\n",
        "\n",
        "\n",
        "## Bayesian versus Frequentist {auto-animate=true background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "**Frequentist:**\n",
        "$$\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}=\\underset{\\boldsymbol{\\theta}}{\\text{argmax}} \\;\\mathcal{L}(x|\\boldsymbol{\\theta})$$\n",
        "\n",
        "- The true parameter is treated as constant.\n",
        "- Estimate aims to maximize the probability of observing data.\n",
        "\n",
        "## Bayesian versus Frequentist {auto-animate=true background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "**Bayesian:**\n",
        "$$\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}}=\\underset{\\boldsymbol{\\theta}}{\\text{argmax}}  \\;P(\\boldsymbol{\\theta}|x)$$\n",
        "\n",
        "- The true parameter is treated as random.\n",
        "- The estimate is chosen as the posterior mode.\n",
        "\n",
        "## Benefits of Bayes over Frequentist{background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "- The ability to incorporate prior knowledge\n",
        "- More interpretable\n",
        "    - Credible intervals vs. confidence intervals\n",
        "    - Estimation of probability of hypotheses\n",
        "    - Resolves some of the limitations of p-values\n",
        "- More flexible\n",
        "    - Hierarchical models are more straightforward\n",
        "    - Easier to take measurement uncertainty into account\n",
        "    - Non-standard hypothesis testing by probing the posterior\n",
        "    \n",
        "## Drawbacks of the Bayes approach:{background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "- Computationally complex \\& demanding\n",
        "- Prior specification is very important\n",
        "    - Too strong of a prior can bias results\n",
        "    - Can affect tractability (remedied by conjugate priors)\n",
        "- Not what people are used to seeing\n",
        "\n",
        "## STAN{background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "- Probabilistic programming language for facilitating Bayesian inference \n",
        "- Has interfaces for R, Python, shell, MATLAB, Julia, and Stata\n",
        "- Multithreaded and compiles down to C++\n",
        "- Intuitive model specification \n",
        "  - e.g:  `y[i] ~ normal(mu,sigma);`\n",
        "- Amazing [reference](https://mc-stan.org/docs/reference-manual/language.html) and [user's guide](https://mc-stan.org/docs/2_19/stan-users-guide/)\n",
        "\n",
        "## How it works: HMC and NUTS {background-image=\"HAM.gif\" background-opacity=\"0.15\" background-size=\"50%\" background-position=\"100% 0%\"}\n",
        "\n",
        "- Hamiltonian Markov Chain (HMC) is an alternative to Metropolis-Hastings or Gibbs Sampling\n",
        "  - Allows for more efficient sampling of high-dimensional spaces\n",
        "  - Analogous to [simulating a particle](https://chi-feng.github.io/mcmc-demo/app.html) moving over the posterior density\n",
        "- The No U-Turns Sampler (NUTS) provides an implementation of HMC which automatically adapts the number of leap-frog steps\n",
        "\n",
        "## Program Structure {auto-animate=true background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "\n",
        "```{stan}\n",
        "data {\n",
        "  // Define the inputs\n",
        "}\n",
        "\n",
        "parameters {\n",
        "  // Define the outputs\n",
        "}\n",
        "\n",
        "model {\n",
        "  // Define the model\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "## Program Structure {auto-animate=true background-image=\"logo_tm.png\" background-opacity=\"0.2\" background-size=\"40%\" background-position=\"0% 0%\"}\n",
        "\n",
        "\n",
        "```{stan}\n",
        "data {\n",
        "  // Define the inputs\n",
        "  int n;\n",
        "  real y[n];\n",
        "}\n",
        "\n",
        "parameters {\n",
        "  // Define the outputs\n",
        "  real mu;\n",
        "  real<lower=0> sigma;\n",
        "}\n",
        "\n",
        "model {\n",
        "  // Define the model\n",
        "  for (i in 1:n)\n",
        "   y[i] ~ normal(mu,sigma);\n",
        "  mu ~ normal(1.7,0.3);\n",
        "  sigma ~ cauchy(0,1);\n",
        "}\n",
        "```"
      ],
      "id": "e0420e0d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}